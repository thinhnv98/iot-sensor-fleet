version: '3.8'

services:
  # Zookeeper
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    hostname: zookeeper
    container_name: zookeeper
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: echo srvr | nc zookeeper 2181 || exit 1
      interval: 10s
      retries: 10
      start_period: 10s
      timeout: 10s

  # Kafka Broker
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    hostname: kafka
    container_name: kafka
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092"
      - "9101:9101"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka:29092,PLAINTEXT_HOST://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: localhost
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
    healthcheck:
      test: kafka-topics --bootstrap-server kafka:29092 --list || exit 1
      interval: 10s
      retries: 20
      start_period: 10s
      timeout: 10s

  # Schema Registry (kept for compatibility with existing services)
  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.0
    hostname: schema-registry
    container_name: schema-registry
    depends_on:
      kafka:
        condition: service_healthy
    ports:
      - "8081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: 'kafka:29092'
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:8081
    healthcheck:
      test: curl --silent --fail http://schema-registry:8081/subjects || exit 1
      interval: 10s
      retries: 20
      start_period: 10s
      timeout: 10s

  # Kafka Connect
  kafka-connect:
    image: confluentinc/cp-kafka-connect:7.5.0
    hostname: kafka-connect
    container_name: kafka-connect
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'kafka:29092'
      CONNECT_REST_ADVERTISED_HOST_NAME: kafka-connect
      CONNECT_REST_PORT: 8083
      CONNECT_GROUP_ID: compose-connect-group
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: "false"
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      CONNECT_LOG4J_LOGGERS: org.apache.zookeeper=ERROR,org.I0Itec.zkclient=ERROR,org.reflections=ERROR
    command:
      - bash
      - -c
      - |
        echo "Installing connectors"
        confluent-hub install --no-prompt confluentinc/kafka-connect-jdbc:10.7.0
        confluent-hub install --no-prompt confluentinc/kafka-connect-elasticsearch:14.0.6
        confluent-hub install --no-prompt confluentinc/kafka-connect-s3:10.5.0
        #
        echo "Launching Kafka Connect worker"
        /etc/confluent/docker/run &
        #
        sleep 30
        #
        echo "Creating connectors"
        # PostgreSQL Sink
        curl -X POST http://kafka-connect:8083/connectors -H "Content-Type: application/json" -d '{
          "name": "postgres-sink",
          "config": {
            "connector.class": "io.confluent.connect.jdbc.JdbcSinkConnector",
            "tasks.max": "1",
            "topics": "sensor.raw",
            "connection.url": "jdbc:postgresql://postgres:5432/sensordb",
            "connection.user": "postgres",
            "connection.password": "postgres",
            "auto.create": "true",
            "auto.evolve": "true",
            "insert.mode": "insert",
            "pk.mode": "none",
            "table.name.format": "sensor_readings",
            "key.converter": "org.apache.kafka.connect.storage.StringConverter",
            "value.converter": "org.apache.kafka.connect.json.JsonConverter",
            "value.converter.schemas.enable": "false"
          }
        }'
        # Elasticsearch Sink
        curl -X POST http://kafka-connect:8083/connectors -H "Content-Type: application/json" -d '{
          "name": "elasticsearch-sink",
          "config": {
            "connector.class": "io.confluent.connect.elasticsearch.ElasticsearchSinkConnector",
            "tasks.max": "1",
            "topics": "sensor.raw",
            "connection.url": "http://elasticsearch:9200",
            "type.name": "_doc",
            "key.ignore": "true",
            "schema.ignore": "false",
            "behavior.on.null.values": "ignore",
            "behavior.on.malformed.documents": "warn",
            "drop.invalid.message": "false",
            "write.method": "insert",
            "max.buffered.records": "20000",
            "batch.size": "2000",
            "max.in.flight.requests": "5",
            "flush.timeout.ms": "30000",
            "linger.ms": "1000",
            "max.retries": "5",
            "retry.backoff.ms": "1000",
            "topics.index.map": "sensor.raw:sensor_readings",
            "key.converter": "org.apache.kafka.connect.storage.StringConverter",
            "value.converter": "org.apache.kafka.connect.json.JsonConverter",
            "value.converter.schemas.enable": "false"
          }
        }'
        # S3 Sink (MinIO)
        curl -X POST http://kafka-connect:8083/connectors -H "Content-Type: application/json" -d '{
          "name": "s3-sink",
          "config": {
            "connector.class": "io.confluent.connect.s3.S3SinkConnector",
            "tasks.max": "1",
            "topics": "sensor.raw",
            "s3.region": "us-east-1",
            "s3.bucket.name": "sensor-cold",
            "s3.part.size": "5242880",
            "flush.size": "1000",
            "storage.class": "io.confluent.connect.s3.storage.S3Storage",
            "format.class": "io.confluent.connect.s3.format.json.JsonFormat",
            "schema.compatibility": "NONE",
            "partitioner.class": "io.confluent.connect.storage.partitioner.DefaultPartitioner",
            "key.converter": "org.apache.kafka.connect.storage.StringConverter",
            "value.converter": "org.apache.kafka.connect.json.JsonConverter",
            "value.converter.schemas.enable": "false",
            "s3.credentials.provider.class": "com.amazonaws.auth.DefaultAWSCredentialsProviderChain",
            "aws.access.key.id": "minioadmin",
            "aws.secret.access.key": "minioadmin",
            "store.url": "http://minio:9000"
          }
        }'
        #
        sleep infinity
    healthcheck:
      test: curl -s -f http://kafka-connect:8083/ || exit 1
      interval: 30s
      retries: 10
      start_period: 30s
      timeout: 10s

  # PostgreSQL
  postgres:
    image: postgres:15-alpine
    hostname: postgres
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      POSTGRES_USER: postgres
      POSTGRES_PASSWORD: postgres
      POSTGRES_DB: sensordb
    volumes:
      - postgres-data:/var/lib/postgresql/data
      - ./postgres/initdb:/docker-entrypoint-initdb.d
    healthcheck:
      test: pg_isready -U postgres -d sensordb
      interval: 10s
      timeout: 5s
      retries: 5

  # Elasticsearch
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.8.0
    hostname: elasticsearch
    container_name: elasticsearch
    ports:
      - "9200:9200"
      - "9300:9300"
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    volumes:
      - elasticsearch-data:/usr/share/elasticsearch/data
    healthcheck:
      test: curl -s http://elasticsearch:9200/_cluster/health | grep -q '"status":"green"'
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 30s

  # MinIO (S3)
  minio:
    image: minio/minio:latest
    hostname: minio
    container_name: minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin
    command: server /data --console-address ":9001"
    volumes:
      - minio-data:/data
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9000/minio/health/live || exit 1"]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 30s

  # MinIO Setup (create bucket)
  minio-setup:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      set -e;
      /usr/bin/mc alias set myminio http://minio:9000 minioadmin minioadmin;
      /usr/bin/mc mb myminio/sensor-cold || echo 'Bucket already exists';
      "

  # Database Setup is now handled by initdb.sql in the postgres service
  # db-setup service has been removed in favor of using PostgreSQL's built-in initialization

  # Prometheus
  prometheus:
    image: prom/prometheus:v2.45.0
    hostname: prometheus
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus-data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/usr/share/prometheus/console_libraries'
      - '--web.console.templates=/usr/share/prometheus/consoles'
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Grafana
  grafana:
    image: grafana/grafana:10.0.3
    hostname: grafana
    container_name: grafana
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - ./grafana/provisioning:/etc/grafana/provisioning
      - grafana-data:/var/lib/grafana
    depends_on:
      prometheus:
        condition: service_healthy
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # Kafka UI
  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    hostname: kafka-ui
    container_name: kafka-ui
    ports:
      - "8080:8080"
    environment:
      - KAFKA_CLUSTERS_0_NAME=local
      - KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS=kafka:29092
      - KAFKA_CLUSTERS_0_ZOOKEEPER=zookeeper:2181
      - KAFKA_CLUSTERS_0_SCHEMAREGISTRY=http://schema-registry:8081
      - KAFKA_CLUSTERS_0_KAFKACONNECT_0_NAME=kafka-connect
      - KAFKA_CLUSTERS_0_KAFKACONNECT_0_ADDRESS=http://kafka-connect:8083
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      kafka-connect:
        condition: service_healthy

  # Sensor Producer
  sensor-producer:
    build:
      context: ..
      dockerfile: Dockerfile
      target: sensor-producer
    container_name: sensor-producer
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      postgres:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    env_file: ../.env
    environment:
      KAFKA_BROKERS: kafka:29092
      SCHEMA_REGISTRY_URL: http://schema-registry:8081
      POSTGRES_HOST: postgres
      ELASTICSEARCH_URL: http://elasticsearch:9200
      MINIO_ENDPOINT: minio:9000
    ports:
      - "2112:2112"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:2112/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # Anomaly Detector
  anomaly-detector:
    build:
      context: ..
      dockerfile: Dockerfile
      target: anomaly-detector
    container_name: anomaly-detector
    depends_on:
      kafka:
        condition: service_healthy
      schema-registry:
        condition: service_healthy
      sensor-producer:
        condition: service_healthy
      postgres:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    env_file: ../.env
    environment:
      KAFKA_BROKERS: kafka:29092
      SCHEMA_REGISTRY_URL: http://schema-registry:8081
      POSTGRES_HOST: postgres
      ELASTICSEARCH_URL: http://elasticsearch:9200
      MINIO_ENDPOINT: minio:9000
      METRICS_PORT: 2113
    ports:
      - "2113:2113"
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:2113/metrics"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

volumes:
  postgres-data:
  elasticsearch-data:
  minio-data:
  prometheus-data:
  grafana-data:
